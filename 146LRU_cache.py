# TODO: learning - Dict+ DoubleLinked list is faster
#  We don't necessarily have to implement Double linked list from scratch(no duplicates) , just use before/next dicts
#  + Learn to use OrderedDict in python
#  move_to_end() and pop_item(last=False)

# Runtime: 1728 ms, faster than 23.63% of Python3 online submissions for LRU Cache.
# Memory Usage: 75.5 MB, less than 60.06% of Python3 online submissions for LRU Cache.

from collections import OrderedDict
class LRUCache:
    def __init__(self, Capacity):
        self.size = Capacity
        self.cache = OrderedDict()

    def get(self, key):
        if key not in self.cache: return -1
        val = self.cache[key]
        self.cache.move_to_end(key)
        return val

    def put(self, key, val):
        if key in self.cache: del self.cache[key]
        self.cache[key] = val
        if len(self.cache) > self.size:
            self.cache.popitem(last=False)

# Runtime: 889 ms, faster than 91.48% of Python3 online submissions for LRU Cache.
# Memory Usage: 74.8 MB, less than 86.79% of Python3 online submissions for LRU Cache.
class LRUCache:
    def __init__(self, MSize):
        self.size = MSize
        self.cache = {}
        self.next, self.before = {}, {}
        self.head, self.tail = '#', '$'
        self.connect(self.head, self.tail)

    def connect(self, a, b):
        self.next[a], self.before[b] = b, a

    def delete(self, key):
        self.connect(self.before[key], self.next[key])
        del self.before[key], self.next[key], self.cache[key]

    def append(self, k, v):
        self.cache[k] = v
        self.connect(self.before[self.tail], k)
        self.connect(k, self.tail)
        if len(self.cache) > self.size:
            self.delete(self.next[self.head])

    def get(self, key):
        if key not in self.cache: return -1
        val = self.cache[key]
        self.delete(key)
        self.append(key, val)
        return val

    def put(self, key, value):
        if key in self.cache: self.delete(key)
        self.append(key, value)